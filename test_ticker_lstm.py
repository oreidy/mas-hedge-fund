"""
Test script for the improved ticker LSTM implementation.

This script validates that the new approach eliminates look-ahead bias and works correctly
with the existing episode data.

Usage:
    python test_ticker_lstm.py --data-dir training_data/episodes
"""

import argparse
import json
import sys
import os
from pathlib import Path
from datetime import datetime

# Add src to path for imports
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import torch
import numpy as np
import pandas as pd
from typing import List, Dict, Any

from src.rl.data_collection.episode import TrainingEpisode
from src.rl.preprocessing.ticker_preprocessor import TickerPreprocessor
from src.rl.models.ticker_lstm_model import TickerLSTMModel
from src.rl.training.ticker_trainer import TickerLSTMTrainer
from utils.logger import setup_logger, logger


def load_episodes_from_directory(data_dir: str, max_episodes: int = None) -> List[TrainingEpisode]:
    """Load training episodes from JSON files."""
    data_path = Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"Data directory not found: {data_dir}")
    
    episode_files = list(data_path.glob("episode_*.json"))
    if not episode_files:
        raise FileNotFoundError(f"No episode files found in {data_dir}")
    
    # Limit episodes for testing
    if max_episodes:
        episode_files = episode_files[:max_episodes]
    
    logger.info(f"Loading {len(episode_files)} episode files for testing")
    
    episodes = []
    for file_path in episode_files:
        try:
            with open(file_path, 'r') as f:
                episode_data = json.load(f)
            
            episode = TrainingEpisode.from_dict(episode_data)
            episodes.append(episode)
            
        except Exception as e:
            logger.warning(f"Failed to load episode from {file_path}: {e}")
            continue
    
    if not episodes:
        raise ValueError("No valid episodes loaded")
    
    # Sort by date
    episodes.sort(key=lambda ep: ep.date)
    
    logger.info(f"Successfully loaded {len(episodes)} episodes from {episodes[0].date} to {episodes[-1].date}")
    return episodes


def test_preprocessor():
    """Test the TickerPreprocessor functionality."""
    logger.info("=== Testing TickerPreprocessor ===")
    
    # Test basic initialization
    preprocessor = TickerPreprocessor(
        sequence_length=5,  # Small for testing
        min_consensus_strength=0.3,
        min_episodes_per_ticker=2
    )
    
    logger.info("âœ“ Preprocessor initialized successfully")
    
    # Test signal mappings
    assert preprocessor.signal_mapping['bullish'] == 1.0
    assert preprocessor.signal_mapping['bearish'] == -1.0
    assert preprocessor.signal_mapping['neutral'] == 0.0
    
    logger.info("âœ“ Signal mappings correct")
    
    return preprocessor


def test_model():
    """Test the TickerLSTMModel functionality."""
    logger.info("=== Testing TickerLSTMModel ===")
    
    # Test model creation
    model = TickerLSTMModel(
        input_size=10,
        hidden_size=32,
        num_layers=1,
        dropout=0.1
    )
    
    logger.info("âœ“ Model initialized successfully")
    
    # Test ticker vocabulary
    test_tickers = {'AAPL', 'MSFT', 'GOOGL'}
    model.build_ticker_vocabulary(test_tickers)
    
    assert len(model.ticker_vocab) == 3
    assert model.ticker_embedding is not None
    
    logger.info("âœ“ Ticker vocabulary built successfully")
    
    # Test forward pass
    batch_size = 2
    sequence_length = 5
    x = torch.randn(batch_size, sequence_length, 10).to(model.device)
    tickers = ['AAPL', 'MSFT']
    
    output = model(x, tickers)
    assert output.shape == (batch_size, 1)
    
    logger.info("âœ“ Forward pass successful")
    
    return model


def test_data_processing(episodes: List[TrainingEpisode]):
    """Test data processing with real episodes."""
    logger.info("=== Testing Data Processing ===")
    
    if len(episodes) < 10:
        logger.warning("Not enough episodes for comprehensive testing")
        return None, None, None
    
    # Use subset for testing
    test_episodes = episodes[:50]  # First 50 episodes
    
    preprocessor = TickerPreprocessor(
        sequence_length=5,
        min_consensus_strength=0.3,
        min_episodes_per_ticker=2
    )
    
    # Test consensus ticker extraction
    consensus_tickers = preprocessor.get_consensus_tickers_from_episodes(test_episodes)
    logger.info(f"âœ“ Found {len(consensus_tickers)} consensus tickers: {list(consensus_tickers)[:10]}...")
    
    # Test training data creation
    training_data = preprocessor.create_ticker_training_data(test_episodes)
    logger.info(f"âœ“ Created {len(training_data)} training samples")
    
    if training_data:
        sample = training_data[0]
        logger.info(f"âœ“ Sample features: {len(sample['features'])} features")
        logger.info(f"âœ“ Sample ticker: {sample['ticker']}")
        logger.info(f"âœ“ Sample target: {sample['target']:.6f}")
    
    # Test fit and transform
    try:
        X, y, metadata = preprocessor.fit_transform(test_episodes)
        
        if len(X) > 0:
            logger.info(f"âœ“ Transform successful: X.shape={X.shape}, y.shape={y.shape}")
            logger.info(f"âœ“ Feature names: {len(preprocessor.feature_names)} features")
            logger.info(f"âœ“ Historical consensus tickers: {len(preprocessor.historical_consensus_tickers)}")
        else:
            logger.warning("No sequences generated - may need to adjust parameters")
        
        return preprocessor, X, metadata
        
    except Exception as e:
        logger.error(f"Transform failed: {e}")
        return None, None, None


def test_training(episodes: List[TrainingEpisode]):
    """Test the training process with a small model."""
    logger.info("=== Testing Training Process ===")
    
    if len(episodes) < 20:
        logger.warning("Not enough episodes for training test")
        return
    
    # Use subset for fast testing
    test_episodes = episodes[:30]
    
    # Create preprocessor
    preprocessor = TickerPreprocessor(
        sequence_length=3,  # Very small for fast testing
        min_consensus_strength=0.2,  # Lower threshold for more data
        min_episodes_per_ticker=1
    )
    
    # Fit and transform
    X, y, metadata = preprocessor.fit_transform(test_episodes)
    
    if len(X) == 0:
        logger.warning("No training data generated for training test")
        return
    
    logger.info(f"âœ“ Generated {len(X)} samples for training test")
    
    # Create small model
    model = TickerLSTMModel(
        input_size=X.shape[2],
        hidden_size=16,  # Very small
        num_layers=1,
        dropout=0.1
    )
    
    # Build vocabulary
    all_tickers = set(meta['ticker'] for meta in metadata)
    model.build_ticker_vocabulary(all_tickers)
    
    logger.info(f"âœ“ Model created with {len(all_tickers)} tickers in vocabulary")
    
    # Create trainer
    trainer = TickerLSTMTrainer(
        model=model,
        preprocessor=preprocessor,
        learning_rate=0.01,
        batch_size=min(8, len(X)),
        max_epochs=3,  # Very few epochs for testing
        patience=2,
        validation_split=0.3
    )
    
    # Train
    try:
        results = trainer.train(test_episodes)
        logger.info(f"âœ“ Training completed in {results['epochs_trained']} epochs")
        logger.info(f"âœ“ Best validation loss: {results['best_val_loss']:.6f}")
        
        return trainer
        
    except Exception as e:
        logger.error(f"Training test failed: {e}")
        return None


def test_temporal_validation():
    """Test temporal validation logic."""
    logger.info("=== Testing Temporal Validation ===")
    
    # Create mock data with dates
    dates = ['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04', '2022-01-05']
    metadata = [{'end_date': date, 'ticker': f'STOCK{i}'} for i, date in enumerate(dates)]
    
    X = np.random.randn(5, 3, 10)
    y = np.random.randn(5)
    
    # Create trainer for testing
    model = TickerLSTMModel(input_size=10, hidden_size=8, num_layers=1)
    preprocessor = TickerPreprocessor()
    
    trainer = TickerLSTMTrainer(
        model=model,
        preprocessor=preprocessor,
        validation_split=0.4  # 40% for validation
    )
    
    # Test temporal split
    train_data, val_data = trainer.create_temporal_splits(X, y, metadata)
    
    X_train, y_train, tickers_train = train_data
    X_val, y_val, tickers_val = val_data
    
    assert len(X_train) == 3  # 60% of 5 = 3
    assert len(X_val) == 2    # 40% of 5 = 2
    
    logger.info(f"âœ“ Temporal split: {len(X_train)} train, {len(X_val)} validation")
    logger.info("âœ“ Temporal validation logic working correctly")


def test_look_ahead_bias_elimination(episodes: List[TrainingEpisode]):
    """Test that look-ahead bias has been eliminated."""
    logger.info("=== Testing Look-Ahead Bias Elimination ===")
    
    if len(episodes) < 5:
        logger.warning("Not enough episodes for bias testing")
        return
    
    test_episodes = episodes[:10]
    
    preprocessor = TickerPreprocessor(
        sequence_length=3,
        min_consensus_strength=0.2,
        min_episodes_per_ticker=1
    )
    
    # Test feature extraction uses only pre-trade data
    episode = test_episodes[0]
    
    # Check if portfolio_after is used in feature extraction (it shouldn't be)
    features = preprocessor.extract_pretrade_features(episode, 'AAPL')
    
    # Ensure features only use 'before' states
    pretrade_features = [key for key in features.keys() if 'before' in key]
    posttrade_features = [key for key in features.keys() if 'after' in key and 'before' not in key]
    
    logger.info(f"âœ“ Pre-trade features: {len(pretrade_features)}")
    logger.info(f"âœ“ Post-trade features: {len(posttrade_features)}")
    
    # The new implementation should have minimal or no post-trade features
    if len(posttrade_features) == 0:
        logger.info("âœ“ No post-trade features found - look-ahead bias eliminated!")
    else:
        logger.warning(f"Found {len(posttrade_features)} post-trade features: {posttrade_features}")
    
    # Test temporal ordering in training data creation
    training_data = preprocessor.create_ticker_training_data(test_episodes)
    
    if training_data:
        # Check that target returns are from next episode
        sample = training_data[0]
        current_date = sample['date']
        next_date = sample['next_date']
        
        logger.info(f"âœ“ Sample uses date {current_date} to predict {next_date}")
        
        # Verify temporal ordering
        current_dt = datetime.strptime(current_date, '%Y-%m-%d')
        next_dt = datetime.strptime(next_date, '%Y-%m-%d')
        
        assert next_dt > current_dt, "Target date should be after current date"
        logger.info("âœ“ Temporal ordering verified - targets are from future episodes")


def main():
    parser = argparse.ArgumentParser(description="Test improved ticker LSTM implementation")
    parser.add_argument('--data-dir', type=str, default='training_data/episodes',
                       help='Directory containing episode JSON files')
    parser.add_argument('--max-episodes', type=int, default=100,
                       help='Maximum number of episodes to load for testing')
    parser.add_argument('--debug', action='store_true',
                       help='Enable debug logging')
    
    args = parser.parse_args()
    
    # Setup logging
    setup_logger(debug_mode=args.debug)
    
    logger.info("Starting ticker LSTM implementation tests")
    
    try:
        # Test 1: Basic component tests
        test_preprocessor()
        test_model()
        test_temporal_validation()
        
        # Test 2: Real data tests
        episodes = load_episodes_from_directory(args.data_dir, args.max_episodes)
        
        preprocessor, X, metadata = test_data_processing(episodes)
        
        if preprocessor is not None and X is not None and len(X) > 0:
            test_look_ahead_bias_elimination(episodes)
            trainer = test_training(episodes)
            
            if trainer is not None:
                logger.info("âœ“ All training tests passed")
        
        logger.info("ðŸŽ‰ All tests completed successfully!")
        logger.info("The improved ticker LSTM implementation is working correctly.")
        
    except Exception as e:
        logger.error(f"Tests failed: {e}")
        raise


if __name__ == "__main__":
    main()